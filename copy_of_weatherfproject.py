# -*- coding: utf-8 -*-
"""Copy of WeatherFproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-XdSpBorqXRTLwWb2thpit4g7d1Dcv-w
"""

import pandas as pd

weather = pd.read_csv("/content/drive/MyDrive/MLworkshop/local_weather.csv", index_col="DATE")#change the file path here

weather
weather.apply(pd.isnull).sum()/weather.shape[0]

core_weather = weather[["PRCP", "SNOW", "SNWD", "TMAX", "TMIN"]].copy()
core_weather.columns = ["precip", "snow", "snow_depth", "temp_max", "temp_min"]

core_weather.apply(pd.isnull).sum()

core_weather["snow"].value_counts()

core_weather["snow_depth"].value_counts()

del core_weather["snow"]
del core_weather["snow_depth"]

core_weather[pd.isnull(core_weather["precip"])]

core_weather.loc["2013-12-15",:]

core_weather["precip"].value_counts() / core_weather.shape[0]

core_weather["precip"] = core_weather["precip"].fillna(0)

core_weather[pd.isnull(core_weather["temp_min"])]

core_weather.loc["2011-12-18":"2011-12-28"]

core_weather = core_weather.fillna(method="ffill")

core_weather.apply(pd.isnull).sum()

# Check for missing value defined in data documentation
core_weather.apply(lambda x: (x == 9999).sum())

core_weather.dtypes

core_weather.index

core_weather.index = pd.to_datetime(core_weather.index)

core_weather.index

core_weather.index.year

core_weather[["temp_max", "temp_min"]].plot()

core_weather.index.year.value_counts().sort_index()

core_weather["precip"].plot()

core_weather.groupby(core_weather.index.year).apply(lambda x: x["precip"].sum()).plot()

core_weather["target"] = core_weather.shift(-1)["temp_max"]
core_weather

core_weather = core_weather.iloc[:-1,:].copy()
core_weather

from sklearn.linear_model import Ridge

reg = Ridge(alpha=.1)

predictors = ["precip", "temp_max", "temp_min"]

train = core_weather.loc[:"2020-12-31"]
test = core_weather.loc["2021-01-01":]

train

test

reg.fit(train[predictors], train["target"])

predictions = reg.predict(test[predictors])

from sklearn.metrics import mean_squared_error

mean_squared_error(test["target"], predictions)

combined = pd.concat([test["target"], pd.Series(predictions, index=test.index)], axis=1)
combined.columns = ["actual", "predictions"]

combined

combined.plot()

reg.coef_

core_weather["month_max"] = core_weather["temp_max"].rolling(30).mean()

core_weather["month_day_max"] = core_weather["month_max"] / core_weather["temp_max"]

core_weather["max_min"] = core_weather["temp_max"] / core_weather["temp_min"]

core_weather = core_weather.iloc[30:,:].copy()

def create_predictions(predictors, core_weather, reg):
    train = core_weather.loc[:"2020-12-31"]
    test = core_weather.loc["2021-01-01":]

    reg.fit(train[predictors], train["target"])
    predictions = reg.predict(test[predictors])

    error = mean_squared_error(test["target"], predictions)

    combined = pd.concat([test["target"], pd.Series(predictions, index=test.index)], axis=1)
    combined.columns = ["actual", "predictions"]
    return error, combined

predictors = ["precip", "temp_max", "temp_min", "month_day_max", "max_min"]

error, combined = create_predictions(predictors, core_weather, reg)
error

combined.plot()

core_weather["monthly_avg"] = core_weather["temp_max"].groupby(core_weather.index.month).apply(lambda x: x.expanding(1).mean())
core_weather["day_of_year_avg"] = core_weather["temp_max"].groupby(core_weather.index.day_of_year).apply(lambda x: x.expanding(1).mean())

error, combined = create_predictions(predictors + ["monthly_avg", "day_of_year_avg"], core_weather, reg)
error

reg.coef_
core_weather.corr()["target"]
combined["diff"] = (combined["actual"] - combined["predictions"]).abs()
combined.sort_values("diff", ascending=False).head(10)

import pandas as pd
weather = pd.read_csv("/content/drive/MyDrive/MLworkshop/weather.csv", index_col="DATE") #change the file path.
weather

null_pct = weather.apply(pd.isnull).sum()/weather.shape[0]
null_pct
valid_columns = weather.columns[null_pct < .05]
valid_columns
weather = weather[valid_columns].copy()
weather.columns = weather.columns.str.lower()
weather
weather = weather.ffill()
weather.apply(pd.isnull).sum()
weather.apply(lambda x: (x == 9999).sum())
weather.dtypes
weather.index
weather.index = pd.to_datetime(weather.index)
weather.index.year.value_counts().sort_index()

weather["snwd"].plot()
weather["target"] = weather.shift(-1)["tmax"]
weather

weather = weather.ffill()
weather
from sklearn.linear_model import Ridge

rr = Ridge(alpha=.1)

predictors = weather.columns[~weather.columns.isin(["target", "name", "station"])]

def backtest(weather, model, predictors, start=3650, step=90):
    all_predictions = []

    for i in range(start, weather.shape[0], step):
        train = weather.iloc[:i,:]
        test = weather.iloc[i:(i+step),:]

        model.fit(train[predictors], train["target"])

        preds = model.predict(test[predictors])
        preds = pd.Series(preds, index=test.index)
        combined = pd.concat([test["target"], preds], axis=1)
        combined.columns = ["actual", "prediction"]
        combined["diff"] = (combined["prediction"] - combined["actual"]).abs()

        all_predictions.append(combined)
    return pd.concat(all_predictions)

predictions = backtest(weather, rr, predictors)

from sklearn.metrics import mean_absolute_error, mean_squared_error

mean_absolute_error(predictions["actual"], predictions["prediction"])

predictions.sort_values("diff", ascending=False)
pd.Series(rr.coef_, index=predictors)

def pct_diff(old, new):
    return (new - old) / old

def compute_rolling(weather, horizon, col):
    label = f"rolling_{horizon}_{col}"
    weather[label] = weather[col].rolling(horizon).mean()
    weather[f"{label}_pct"] = pct_diff(weather[label], weather[col])
    return weather

rolling_horizons = [3, 14]
for horizon in rolling_horizons:
    for col in ["tmax", "tmin", "prcp"]:
        weather = compute_rolling(weather, horizon, col)

def expand_mean(df):
    return df.expanding(1).mean()

for col in ["tmax", "tmin", "prcp"]:
    weather[f"month_avg_{col}"] = weather[col].groupby(weather.index.month, group_keys=False).apply(expand_mean)
    weather[f"day_avg_{col}"] = weather[col].groupby(weather.index.day_of_year, group_keys=False).apply(expand_mean)

def pct_diff(old, new):
  return (new-old)/ old

  def computerolling(weather, horizon, col):

    label=f"rolling{horizon}_{col}"

    weather[label]=weather[col].rolling(horizon).mean()

    weather[f"{label}_pct"]= pct_diff(weather[label], weather[col])

    return weather
  rolling_horizons=[3,14]
  for horizon in rolling_horizons:
    for col in["tmax","tmin","prcp"]:
      weather= compute_rolling(weather, horizon, col)

def expand_mean(df):
    return df.expanding(1).mean()

for col in ["tmax", "tmin", "prcp"]:
    weather[f"month_avg_{col}"] = weather[col].groupby(weather.index.month, group_keys=False).apply(expand_mean)
    weather[f"day_avg_{col}"] = weather[col].groupby(weather.index.day_of_year, group_keys=False).apply(expand_mean)
weather = weather.iloc[14:,:]
weather = weather.fillna(0)

predictors = weather.columns[~weather.columns.isin(["target", "name", "station"])]

predictions = backtest(weather, rr, predictors)
mean_absolute_error(predictions["actual"], predictions["prediction"])

predictions.sort_values("diff", ascending=False)

weather.loc["1990-03-07": "1990-03-17"]

(predictions["diff"].round().value_counts().sort_index() / predictions.shape[0]).plot()
predictions